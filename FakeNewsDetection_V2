{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":310019,"sourceType":"datasetVersion","datasetId":129603}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-17T09:55:57.604994Z","iopub.execute_input":"2024-09-17T09:55:57.605445Z","iopub.status.idle":"2024-09-17T09:55:58.106425Z","shell.execute_reply.started":"2024-09-17T09:55:57.605399Z","shell.execute_reply":"2024-09-17T09:55:58.105112Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/textdb3/fake_or_real_news.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cell 1: Imports and Configuration\n\nimport re\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, SimpleRNN, Conv1D, GlobalMaxPooling1D, Dense\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration\nDATA_PATH = '/kaggle/input/textdb3/fake_or_real_news.csv'\nTEST_SIZE = 0.2\nRANDOM_STATE = 42\nBATCH_SIZE = 64\nEPOCHS = 10\nEMBEDDING_DIM = 100\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T09:55:58.109036Z","iopub.execute_input":"2024-09-17T09:55:58.109655Z","iopub.status.idle":"2024-09-17T09:56:16.280545Z","shell.execute_reply.started":"2024-09-17T09:55:58.109573Z","shell.execute_reply":"2024-09-17T09:56:16.279341Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Cell 2: Load and Preprocess the Data\n\n# Load dataset\ndf = pd.read_csv(DATA_PATH)\n\n# Display the first few rows of the dataset\nprint(f\"Dataset Shape: {df.shape}\")\ndf.head()\n\n# Check for null values\nprint(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n\n# Preprocess text (lowercasing, removing URLs, punctuation, etc.)\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n    text = re.sub(r'<.*?>', '', text)                 # Remove HTML tags\n    text = re.sub(r'[^\\w\\s]', '', text)               # Remove punctuation\n    text = re.sub(r'\\d', '', text)                    # Remove digits\n    text = re.sub(r'\\n', ' ', text)                   # Replace newline with space\n    return text.strip()\n\ndf['text'] = df['text'].apply(preprocess_text)\n\n# Map labels to binary (FAKE -> 0, REAL -> 1)\ndf['label'] = df['label'].map({'FAKE': 0, 'REAL': 1})\n\n# Verify the label distribution\nprint(f\"\\nLabel distribution:\\n{df['label'].value_counts()}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T09:56:16.282298Z","iopub.execute_input":"2024-09-17T09:56:16.283212Z","iopub.status.idle":"2024-09-17T09:56:20.430720Z","shell.execute_reply.started":"2024-09-17T09:56:16.283161Z","shell.execute_reply":"2024-09-17T09:56:20.429191Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Dataset Shape: (6335, 4)\n\nMissing values:\nUnnamed: 0    0\ntitle         0\ntext          0\nlabel         0\ndtype: int64\n\nLabel distribution:\nlabel\n1    3171\n0    3164\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cell 3: Train-Test Split and TF-IDF Vectorization\n\n# Split data into features and target\nX = df['text']\ny = df['label']\n\n# Train-test split with stratification\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n\n# TF-IDF Vectorization\nprint(\"Vectorizing text data using TF-IDF...\")\ntfidf_vectorizer = TfidfVectorizer(max_features=10000)\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\nprint(f\"X_train_tfidf shape: {X_train_tfidf.shape}\")\nprint(f\"X_test_tfidf shape: {X_test_tfidf.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T09:56:20.432534Z","iopub.execute_input":"2024-09-17T09:56:20.433035Z","iopub.status.idle":"2024-09-17T09:56:26.958651Z","shell.execute_reply.started":"2024-09-17T09:56:20.432987Z","shell.execute_reply":"2024-09-17T09:56:26.957202Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Vectorizing text data using TF-IDF...\nX_train_tfidf shape: (5068, 10000)\nX_test_tfidf shape: (1267, 10000)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cell 4: Model Definitions and Hyperparameter Tuning\n\n# Define models and hyperparameters\nmodels = {\n    'Logistic Regression': {\n        'model': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n        'params': {\n            'C': [0.01, 0.1, 1, 10],\n            'solver': ['liblinear', 'saga']\n        }\n    },\n    'Random Forest': {\n        'model': RandomForestClassifier(random_state=RANDOM_STATE),\n        'params': {\n            'n_estimators': [100, 200],\n            'max_depth': [None, 10, 20],\n            'min_samples_split': [2, 5]\n        }\n    },\n    'Decision Tree': {\n        'model': DecisionTreeClassifier(random_state=RANDOM_STATE),\n        'params': {\n            'max_depth': [None, 10, 20],\n            'min_samples_split': [2, 5, 10]\n        }\n    },\n    'Gradient Boosting': {\n        'model': GradientBoostingClassifier(random_state=RANDOM_STATE),\n        'params': {\n            'n_estimators': [100, 200],\n            'learning_rate': [0.01, 0.1],\n            'max_depth': [3, 5]\n        }\n    }\n}\n\n# Hyperparameter tuning using GridSearchCV\nbest_models = {}\nfor name, config in models.items():\n    print(f\"\\nTuning hyperparameters for {name}...\")\n    grid = GridSearchCV(config['model'], config['params'], cv=5, scoring='f1', n_jobs=-1, verbose=1)\n    grid.fit(X_train_tfidf, y_train)\n    best_models[name] = grid.best_estimator_\n    print(f\"Best Parameters for {name}: {grid.best_params_}\")\n    print(f\"Best F1 Score: {grid.best_score_:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:23:24.709512Z","iopub.execute_input":"2024-09-17T10:23:24.710016Z","iopub.status.idle":"2024-09-17T10:56:01.479104Z","shell.execute_reply.started":"2024-09-17T10:23:24.709971Z","shell.execute_reply":"2024-09-17T10:56:01.477652Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\nTuning hyperparameters for Logistic Regression...\nFitting 5 folds for each of 8 candidates, totalling 40 fits\nBest Parameters for Logistic Regression: {'C': 10, 'solver': 'saga'}\nBest F1 Score: 0.9278\n\nTuning hyperparameters for Random Forest...\nFitting 5 folds for each of 12 candidates, totalling 60 fits\nBest Parameters for Random Forest: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\nBest F1 Score: 0.8952\n\nTuning hyperparameters for Decision Tree...\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nBest Parameters for Decision Tree: {'max_depth': None, 'min_samples_split': 2}\nBest F1 Score: 0.7939\n\nTuning hyperparameters for Gradient Boosting...\nFitting 5 folds for each of 8 candidates, totalling 40 fits\nBest Parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\nBest F1 Score: 0.9097\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cell 5: Model Evaluation\n\n# Function to calculate metrics\ndef calculate_metrics(y_true, y_pred):\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    return accuracy, precision, recall, f1\n\n# Evaluate each model\nfor name, model in best_models.items():\n    print(f\"\\nEvaluating {name}...\")\n    y_pred = model.predict(X_test_tfidf)\n    acc, precision, recall, f1 = calculate_metrics(y_test, y_pred)\n    print(f\"Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T11:03:25.683839Z","iopub.execute_input":"2024-09-17T11:03:25.684351Z","iopub.status.idle":"2024-09-17T11:03:26.100850Z","shell.execute_reply.started":"2024-09-17T11:03:25.684302Z","shell.execute_reply":"2024-09-17T11:03:26.099662Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\nEvaluating Logistic Regression...\nAccuracy: 0.9408, Precision: 0.9472, Recall: 0.9338, F1 Score: 0.9404\n\nEvaluating Random Forest...\nAccuracy: 0.9124, Precision: 0.9157, Recall: 0.9085, F1 Score: 0.9121\n\nEvaluating Decision Tree...\nAccuracy: 0.7885, Precision: 0.7905, Recall: 0.7855, F1 Score: 0.7880\n\nEvaluating Gradient Boosting...\nAccuracy: 0.9021, Precision: 0.9100, Recall: 0.8927, F1 Score: 0.9013\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cell 6: Deep Learning Models (LSTM, RNN, CNN)\n\n# Tokenization and padding for TensorFlow models\nprint(\"Tokenizing and padding text data for TensorFlow models...\")\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nsequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_test = tokenizer.texts_to_sequences(X_test)\n\nvocab_size = len(tokenizer.word_index) + 1\nmax_length = max(len(seq) for seq in sequences_train)\nX_train_padded = pad_sequences(sequences_train, maxlen=max_length, padding='post')\nX_test_padded = pad_sequences(sequences_test, maxlen=max_length, padding='post')\n\nprint(f\"Vocabulary Size: {vocab_size}\")\nprint(f\"Maximum Sequence Length: {max_length}\")\n\n# Define function to build and compile models\ndef build_model(model_type):\n    model = Sequential()\n    model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n    \n    if model_type == 'LSTM':\n        model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n    elif model_type == 'RNN':\n        model.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2))\n    elif model_type == 'CNN':\n        model.add(Conv1D(128, kernel_size=5, activation='relu'))\n        model.add(GlobalMaxPooling1D())\n    \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# Train and evaluate each model\nfor model_type in ['LSTM', 'RNN', 'CNN']:\n    print(f\"\\nTraining {model_type} model...\")\n    model = build_model(model_type)\n    model.fit(X_train_padded, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test_padded, y_test), verbose=2)\n    \n    print(f\"\\nEvaluating {model_type} model...\")\n    loss, accuracy = model.evaluate(X_test_padded, y_test, verbose=0)\n    print(f\"{model_type} Model Accuracy: {accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T11:03:42.259910Z","iopub.execute_input":"2024-09-17T11:03:42.260357Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Tokenizing and padding text data for TensorFlow models...\nVocabulary Size: 81286\nMaximum Sequence Length: 20707\n\nTraining LSTM model...\nEpoch 1/10\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cell 7: Confusion Matrix and Final Evaluation\n\n# Confusion matrix plotting function\ndef plot_confusion_matrix(y_true, y_pred, title):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6,5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['FAKE', 'REAL'], yticklabels=['FAKE', 'REAL'])\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title(title)\n    plt.show()\n\n# Plot confusion matrices for each scikit-learn model\nfor name, model in best_models.items():\n    y_pred = model.predict(X_test_tfidf)\n    plot_confusion_matrix(y_test, y_pred, f\"Confusion Matrix for {name}\")\n\n# Plot confusion matrix for LSTM model\nprint(\"\\nPlotting confusion matrix for LSTM model...\")\nlstm_model = build_model('LSTM')\nlstm_model.fit(X_train_padded, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)\nlstm_pred = (lstm_model.predict(X_test_padded) > 0.5).astype(\"int32\")\nplot_confusion_matrix(y_test, lstm_pred, \"Confusion Matrix for LSTM Model\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:23:18.054847Z","iopub.status.idle":"2024-09-17T10:23:18.055292Z","shell.execute_reply.started":"2024-09-17T10:23:18.055074Z","shell.execute_reply":"2024-09-17T10:23:18.055095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cell 8: Storing Model Metrics for Comparison\n\n# Dictionary to store metrics for all models\nmodel_metrics = {}\n\n# Scikit-learn Models\nfor name, model in best_models.items():\n    y_pred = model.predict(X_test_tfidf)\n    accuracy, precision, recall, f1 = calculate_metrics(y_test, y_pred)\n    model_metrics[name] = {\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1 Score': f1\n    }\n    print(f\"\\n{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\n# Deep Learning Models\nfor model_type in ['LSTM', 'RNN', 'CNN']:\n    print(f\"\\nEvaluating {model_type} model...\")\n    model = build_model(model_type)\n    model.fit(X_train_padded, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test_padded, y_test), verbose=0)\n    loss, accuracy = model.evaluate(X_test_padded, y_test, verbose=0)\n    y_pred_dl = (model.predict(X_test_padded) > 0.5).astype(\"int32\")\n    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_dl, average='binary')\n    \n    model_metrics[model_type] = {\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1 Score': f1\n    }\n    print(f\"{model_type} Model - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:23:18.057136Z","iopub.status.idle":"2024-09-17T10:23:18.057742Z","shell.execute_reply.started":"2024-09-17T10:23:18.057424Z","shell.execute_reply":"2024-09-17T10:23:18.057453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cell 9: Plotting the Comparison Between Models\n\nimport matplotlib.pyplot as plt\n\n# Function to plot model comparisons\ndef plot_model_comparison(model_metrics):\n    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n    models = list(model_metrics.keys())\n    \n    # Prepare the data for each metric\n    for metric in metrics:\n        scores = [model_metrics[model][metric] for model in models]\n        \n        # Plot each metric\n        plt.figure(figsize=(10, 6))\n        plt.barh(models, scores, color='skyblue')\n        plt.title(f'Model Comparison - {metric}')\n        plt.xlabel(metric)\n        plt.xlim(0, 1)  # Since metrics are between 0 and 1\n        plt.ylabel('Models')\n        plt.show()\n\n# Plot the model comparison\nplot_model_comparison(model_metrics)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:23:18.059109Z","iopub.status.idle":"2024-09-17T10:23:18.059699Z","shell.execute_reply.started":"2024-09-17T10:23:18.059385Z","shell.execute_reply":"2024-09-17T10:23:18.059414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}